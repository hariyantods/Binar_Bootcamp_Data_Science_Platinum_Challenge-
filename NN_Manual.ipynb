{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29190d22-1642-4e68-8088-242d8d0b0e7b",
   "metadata": {},
   "source": [
    "# Neural Network and Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "Artificial neurons (also called Perceptrons, Units or Nodes) are the simplest elements or building blocks in a neural network. They are inspired by biological neurons that are found in the human brain.\n",
    "\n",
    "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*hkYlTODpjJgo32DoCOWN5w.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Layer in Neural Network\n",
    "\n",
    "In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network. Hidden layers vary depending on the function of the neural network, and similarly, the layers may vary depending on their associated weights.\n",
    "\n",
    "![](https://www.druva.com/assets/blog-graphic_visualizing-neural-networks-2048x840.jpg)\n",
    "\n",
    "## Neural Network Intuition\n",
    "\n",
    "![](https://github.com/BenedictusAryo/konten_image/raw/master/crop_NN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b04308-9f1c-488e-be54-b639220b722f",
   "metadata": {},
   "source": [
    "## Components of Neural Network\n",
    "\n",
    "1. **X** = Input data\n",
    "2. **Y** = Output Data\n",
    "3. **Z** = Hidden Layer\n",
    "4. **W** = Weight that connect between nodes\n",
    "5. **b** = Bias (Constant)\n",
    "6. **a** = Activation Function (Non Linear Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd40c81-0794-467f-b48f-66b5bb8020c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Manual Sample Calculation of Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85a28a93-79e3-4bc9-abee-803d974a6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Input Layer\n",
    "x1 = 0.7\n",
    "x2 = 0.8\n",
    "x3 = 0.9\n",
    "\n",
    "# Output layer\n",
    "yd6 = 0\n",
    "\n",
    "# Initialized weight\n",
    "w14 = 0.5\n",
    "w15 = 0.6\n",
    "w24 = 0.3\n",
    "w25 = 1.1\n",
    "w34 = -1.0\n",
    "w35 = 0.1\n",
    "w46 = -1.1\n",
    "w56 = -0.7\n",
    "\n",
    "# Bias / Theta & threshold\n",
    "thres1 = thres2 = thres3 = -1\n",
    "theta4 = 0.2\n",
    "theta5 = 0.3\n",
    "theta6 = 0.4\n",
    "\n",
    "# Activation Function\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return 1 / (1 + np.exp(-x))* (1 - 1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d1215b5-1753-460d-8916-1abc2de8e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum y4: -0.51, sum y5: 1.09, sum y6: -1.3365800832096726\n",
      "Y4: 0.3751935255315707, Y5: 0.7483817216070642, Y6: 0.20807302520657042\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "sum_y4 = x1*w14 + x2*w24 + x3*w34 + thres1*theta4\n",
    "sum_y5 = x1*w15 + x2*w25 + x3*w35 + thres2*theta5\n",
    "y4 = sigmoid(sum_y4)\n",
    "y5 = sigmoid(sum_y5)\n",
    "sum_y6 = y4*w46 + y5*w56 + thres3*theta6\n",
    "y6 = sigmoid(sum_y6)\n",
    "print(f\"Sum y4: {sum_y4}, sum y5: {sum_y5}, sum y6: {sum_y6}\")\n",
    "print(f\"Y4: {y4}, Y5: {y5}, Y6: {y6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bea1a2f-aca7-4ab4-8400-d51a7b2a00eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20807302520657042"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error calculation\n",
    "error = yd6 - y6\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c87ac193-bf99-4b3e-b2ca-8692fddb5234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03428599040302067"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient error output Y6\n",
    "error_grad6 = y6*(1-y6)*(yd6-y6)\n",
    "error_grad6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb9e51d1-448e-4948-bebc-afab47502b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad4 = -0.0012863881615650925, grad5 = -0.0025659008524815887, grad_bias = -0.001371439616120827\n"
     ]
    }
   ],
   "source": [
    "# Backward Pass output Y6\n",
    "grad46 = alpha * y4 * error_grad6\n",
    "grad56 = alpha * y5 * error_grad6\n",
    "grad_bias6 = alpha * theta6 * error_grad6\n",
    "print(f\"grad4 = {grad46}, grad5 = {grad56}, grad_bias = {grad_bias6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "570c8f19-1205-4656-ab19-3b8f0131c613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad4 = 0.008841180172279505, grad5 = 0.004519392885198686\n"
     ]
    }
   ],
   "source": [
    "# Gradient error output Y4 Y5\n",
    "# Loss Calculation MSE\n",
    "gradY4 = y4*(1-y4)*error_grad6*w46\n",
    "gradY5 = y5*(1-y5)*error_grad6*w56\n",
    "print(f\"grad4 = {gradY4}, grad5 = {gradY5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48f57dda-1074-42ab-ab3f-55768561b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad 14 : 0.0006188826120595653, grad 24 : 0.0007072944137823606, grad 34 : 0.0007957062155051556, grad_bias4 : 0.00017682360344559015\n"
     ]
    }
   ],
   "source": [
    "#Weight connection  Backward Pass output Y4\n",
    "grad14 = alpha * x1 * gradY4\n",
    "grad24 = alpha * x2 * gradY4\n",
    "grad34 = alpha * x3 * gradY4\n",
    "grad_bias4 = alpha * theta4 * gradY4\n",
    "print(f\"grad 14 : {grad14}, grad 24 : {grad24}, grad 34 : {grad34}, grad_bias4 : {grad_bias4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44a9b1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad 15 : 0.00031635750196390796, grad 25 : 0.00036155143081589494, grad 35 : 0.00040674535966788176, grad_bias5 : 0.0002652354051683852\n"
     ]
    }
   ],
   "source": [
    "#Weight connection  Backward Pass output Y5\n",
    "grad15 = alpha * x1 * gradY5\n",
    "grad25 = alpha * x2 * gradY5\n",
    "grad35 = alpha * x3 * gradY5\n",
    "grad_bias5 = alpha * theta5 * gradY4\n",
    "print(f\"grad 15 : {grad15}, grad 25 : {grad25}, grad 35 : {grad35}, grad_bias5 : {grad_bias5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23611e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights and bias:\n",
      "w14 = 0.5006188826120596\n",
      "w15 = 0.6003163575019639\n",
      "w24 = 0.30070729441378236\n",
      "w25 = 1.100361551430816\n",
      "w34 = -0.9992042937844948\n",
      "w35 = 0.10040674535966788\n",
      "w46 = -1.101286388161565\n",
      "w56 = -0.7025659008524815\n",
      "theta4 = 0.2001768236034456\n",
      "theta5 = 0.30026523540516836\n",
      "theta6 = 0.39862856038387917\n"
     ]
    }
   ],
   "source": [
    "#Update vakues \n",
    "w14 = w14 + grad14\n",
    "w15 = w15 + grad15\n",
    "w24 = w24 + grad24\n",
    "w25 = w25 + grad25\n",
    "w34 = w34 + grad34\n",
    "w35 = w35 + grad35\n",
    "w46 = w46 + grad46\n",
    "w56 = w56 + grad56\n",
    "theta4 = theta4 + grad_bias4\n",
    "theta5 = theta5 + grad_bias5\n",
    "theta6 = theta6 + grad_bias6\n",
    "\n",
    "print(\"Updated weights and bias:\")\n",
    "print(\"w14 =\", w14)\n",
    "print(\"w15 =\", w15)\n",
    "print(\"w24 =\", w24)\n",
    "print(\"w25 =\", w25)\n",
    "print(\"w34 =\", w34)\n",
    "print(\"w35 =\", w35)\n",
    "print(\"w46 =\", w46)\n",
    "print(\"w56 =\", w56)\n",
    "print(\"theta4 =\", theta4)\n",
    "print(\"theta5 =\", theta5)\n",
    "print(\"theta6 =\", theta6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11420d99-18d8-4b99-bdac-c7f108ad81ce",
   "metadata": {},
   "source": [
    "## Neural Network Manual using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfdace8-101c-43d5-b7f4-e653acfb7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0538a73-0c3b-47ad-a340-85df2fe769e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, learning_rate=0.1):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
    "        self.weights2 = np.random.rand(4, 1)\n",
    "        self.bias1 = np.zeros((1, 4))\n",
    "        self.bias2 = np.zeros((1, 1))\n",
    "        self.y = y\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def forwardprop(self):\n",
    "        self.layer1 = self.sigmoid(np.dot(self.input, self.weights1) + self.bias1)\n",
    "        self.output = self.sigmoid(np.dot(self.layer1, self.weights2) + self.bias2)\n",
    "\n",
    "    def backprop(self):\n",
    "        error = self.y - self.output\n",
    "        d_output = error * self.sigmoid_derivative(self.output)\n",
    "        error_layer1 = d_output.dot(self.weights2.T)\n",
    "        d_layer1 = error_layer1 * self.sigmoid_derivative(self.layer1)\n",
    "        self.d_output = d_output\n",
    "        self.error_layer1 = error_layer1\n",
    "        \n",
    "    def update_weights(self):\n",
    "        self.weights2 += self.layer1.T.dot(self.d_output) * self.learning_rate\n",
    "        self.weights1 += self.input.T.dot(self.d_layer1) * self.learning_rate\n",
    "        self.bias2 += np.sum(self.d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.bias1 += np.sum(self.d_layer1, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.output = np.zeros(y.shape)\n",
    "        self.input = X\n",
    "        self.y = y\n",
    "        self.forwardprop()\n",
    "        self.backprop()\n",
    "        self.update_weights()\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        error = self.y - self.output\n",
    "        return np.mean(error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7591f318-c8b4-457a-8d52-1a9589b0e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, learning_rate=0.1):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
    "        self.weights2 = np.random.rand(4, 1)\n",
    "        self.bias1 = np.zeros((1, 4))\n",
    "        self.bias2 = np.zeros((1, 1))\n",
    "        self.y = y\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = self.sigmoid(np.dot(self.input, self.weights1) + self.bias1)\n",
    "        self.output = self.sigmoid(np.dot(self.layer1, self.weights2) + self.bias2)\n",
    "\n",
    "    def backprop(self):\n",
    "        error = self.y - self.output\n",
    "        d_output = error * self.sigmoid_derivative(self.output)\n",
    "        error_layer1 = d_output.dot(self.weights2.T)\n",
    "        d_layer1 = error_layer1 * self.sigmoid_derivative(self.layer1)\n",
    "        self.weights2 += self.layer1.T.dot(d_output) * self.learning_rate\n",
    "        self.weights1 += self.input.T.dot(d_layer1) * self.learning_rate\n",
    "        self.bias2 += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.bias1 += np.sum(d_layer1, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.output = np.zeros(y.shape)\n",
    "        self.input = X\n",
    "        self.y = y\n",
    "        self.feedforward()\n",
    "        self.backprop()\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        error = self.y - self.output\n",
    "        return np.mean(error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d3da86-a828-4a3e-9ff1-f83f0c47099d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR Dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "df = pd.DataFrame(X, columns=['x1','x2'])\n",
    "df['y'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fcc6d49-2123-403d-8a67-e177cc34cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.030189432724354603\n",
      "Input: \n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      "[[0.27592769]\n",
      " [0.85657558]\n",
      " [0.84855604]\n",
      " [0.96659637]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(X,y)\n",
    "nn.train(X,y)\n",
    "\n",
    "for i in range(1000):\n",
    "    nn.train(X, y)\n",
    "\n",
    "print(\"Loss: \", nn.calculate_loss())\n",
    "print(\"Input: \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\" + str(nn.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968d864-235d-4698-85ba-060ff52299b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987fc78-37a3-478c-b2d7-30536e91d60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462d2047-e284-47ea-b287-9403209370f4",
   "metadata": {},
   "source": [
    "### Links:\n",
    "* https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53#:~:text=Perceptron%20is%20a%20single%20layer,classify%20the%20given%20input%20data\n",
    "* https://towardsdatascience.com/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc\n",
    "* https://www.druva.com/blog/understanding-neural-networks-through-visualization/\n",
    "* https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056ea64-eeea-4a50-b611-82d28f454d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "195425acf4ac5fe9ab6cbc365d49ab977835af7e3a781715312f610eb17001a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
